{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1d3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ADK components imported successfully.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m     16\u001b[39m cred = credentials.Certificate(\u001b[33m\"\u001b[39m\u001b[33mlanaagent-firebase-adminsdk-fbsvc-3ed45f99b9.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mfirebase_admin\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m db=firestore.client()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Owner\\anaconda3\\envs\\about_Lana_agent\\Lib\\site-packages\\firebase_admin\\__init__.py:74\u001b[39m, in \u001b[36minitialize_app\u001b[39m\u001b[34m(credential, options, name)\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m app\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == _DEFAULT_APP_NAME:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\n\u001b[32m     75\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThe default Firebase app already exists. This means you called \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     76\u001b[39m         \u001b[33m'\u001b[39m\u001b[33minitialize_app() more than once without providing an app name as \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     77\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mthe second argument. In most cases you only need to call \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     78\u001b[39m         \u001b[33m'\u001b[39m\u001b[33minitialize_app() once. But if you do want to initialize multiple \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     79\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mapps, pass a second argument to initialize_app() to give each app \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     80\u001b[39m         \u001b[33m'\u001b[39m\u001b[33ma unique name.\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     83\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFirebase app named \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m already exists. This means you called \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minitialize_app() more than once with the same app name as the \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msecond argument. Make sure you provide a unique name every time \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[33m'\u001b[39m\u001b[33myou call initialize_app().\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name."
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "from google.adk.agents.llm_agent import Agent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools import google_search, AgentTool, ToolContext\n",
    "from google.adk.code_executors import BuiltInCodeExecutor\n",
    "\n",
    "print(\"âœ… ADK components imported successfully.\")\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "cred = credentials.Certificate(\"lanaagent-firebase-adminsdk-fbsvc-3ed45f99b9.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "\n",
    "db=firestore.client()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c03e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers and tools\n",
    "\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=3,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "def log_persistence(log_entry_data: dict, is_authenticated: bool) -> bool:\n",
    "    \"\"\"\n",
    "    Saves a structured log entry to the Google Cloud Persistence Store (Firestore).\n",
    "    Includes a Contact Gate check and an auto-generated timestamp.\n",
    "\n",
    "    It also controls the sharing of contact information based on authentication status.\n",
    "\n",
    "    Args:\n",
    "        log_entry_data (dict): The log entry dictionary with this structure:\n",
    "            {\n",
    "                \"timestamp\": \"ISO 8601\",\n",
    "                \"category\": \"Structural/Time, Persona, Avoidance, or Project\",\n",
    "                \"key\": \"The specific field name (e.g., 'Preferred Communication Tone')\",\n",
    "                \"value\": \"The current data value\",\n",
    "                \"source\": \"e.g., 'User Input', 'Bujo OCR', 'Agent Check'\"\n",
    "            }\n",
    "        is_authenticated (bool): Boolean flag indicating if the user is logged into \n",
    "                          their Google Account (True) or anonymous (False).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the log was successfully written; False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # 1. Structural Check: The Contact Gate (for ethical protection)\n",
    "    if log_entry_data.get('key') == 'contact_info' and not is_authenticated:\n",
    "        print(\"ACCESS DENIED: Cannot share contact info for anonymous user.\")\n",
    "        # We do not log the entry to prevent accidental data leakage\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # 2. Add Required Metadata (Timestamp)\n",
    "        # Create a new dictionary to ensure we always log a timestamp\n",
    "        data_to_log = log_entry_data.copy()\n",
    "        data_to_log['timestamp'] = datetime.utcnow()\n",
    "        \n",
    "        # 3. Cloud Write: Writing the data to the 'logs' collection in Firestore\n",
    "        db.collection('logs').add(data_to_log)\n",
    "        \n",
    "        print(f\"SUCCESS: Logged data to Firestore: {log_entry_data.get('key')}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FAILURE: Could not write to Firestore. Error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81086b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… log_analyst_agent created with custom function tools\n",
      "ðŸ”§ Available tools:\n"
     ]
    }
   ],
   "source": [
    "#tools part 2\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "def query_persistence(task_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Queries the Persistence Store (Firestore) for a specific task's history \n",
    "    and checks if the Contradiction Flag should be raised.\n",
    "    \n",
    "    Args:\n",
    "        task_id: The unique identifier (Task Key) of the task to search for.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the check result and failure count.\n",
    "    \"\"\"\n",
    "    \n",
    "    logs_ref = db.collection('logs').where('key', '==', task_id)\n",
    "    \n",
    "    flag_raised = True if logs_ref is None else False\n",
    "    \n",
    "    # Simulate a successful query that found 4 failures in a row\n",
    "    consecutive_failure_count = 4 \n",
    "\n",
    "    # THE CONTRADICTION FLAG LOGIC \n",
    "    flag_raised = consecutive_failure_count >= 3\n",
    "\n",
    "    return {\n",
    "        \"task_id\": task_id,\n",
    "        \"consecutive_failures\": consecutive_failure_count,\n",
    "        \"flag_raised\": flag_raised\n",
    "    }\n",
    "\n",
    "def _parse_date_token(date_str: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Parse a date string that may be either an ISO date or a relative token.\n",
    "\n",
    "    Supported formats:\n",
    "    - ISO date: 'YYYY-MM-DD' (e.g., '2025-11-24')\n",
    "    - Relative tokens: 'NOW', 'TODAY', 'NOW-7DAYS', 'NOW-30DAYS' (case-insensitive)\n",
    "\n",
    "    Returns a timezone-naive UTC datetime at the start of the day.\n",
    "    Raises ValueError for unsupported formats.\n",
    "    \"\"\"\n",
    "    if not isinstance(date_str, str):\n",
    "        raise ValueError(\"date_str must be a string\")\n",
    "\n",
    "    token = date_str.strip().upper()\n",
    "    if token in (\"NOW\", \"TODAY\"):\n",
    "        return datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "    # Handle patterns like NOW-7DAYS or TODAY-1DAYS\n",
    "    if (token.startswith(\"NOW-\") or token.startswith(\"TODAY-\")) and \"DAYS\" in token:\n",
    "        try:\n",
    "            # last part expected to be like '7DAYS' -> extract leading integer\n",
    "            n_part = token.split(\"-\")[-1]\n",
    "            n = int(''.join(ch for ch in n_part if ch.isdigit()))\n",
    "            return (datetime.utcnow() - timedelta(days=n)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not parse relative date token: {date_str}\") from e\n",
    "\n",
    "    # Fallback: try ISO format\n",
    "    try:\n",
    "        return datetime.strptime(token, \"%Y-%m-%d\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Unsupported date format: {date_str}. Use 'YYYY-MM-DD' or 'NOW-<NDAYS>'\") from e\n",
    "\n",
    "\n",
    "def make_a_report(start_date_str: str, end_date_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Queries the Persistence Store (Firestore) for a date span and returns matching logs.\n",
    "\n",
    "    Accepted date formats:\n",
    "    - ISO date: 'YYYY-MM-DD' (e.g., '2025-11-01')\n",
    "    - Relative tokens: 'NOW', 'TODAY', 'NOW-7DAYS' (case-insensitive)\n",
    "\n",
    "    Args:\n",
    "        start_date_str: Start date string or relative token.\n",
    "        end_date_str: End date string or relative token.\n",
    "\n",
    "    Returns:\n",
    "        dict: { 'logs': [ ... ] }\n",
    "    \"\"\"\n",
    "    # Parse inputs (accept tokens like NOW-7DAYS)\n",
    "    start_date = _parse_date_token(start_date_str)\n",
    "    end_date = _parse_date_token(end_date_str)\n",
    "\n",
    "    # Ensure end_date includes the full day\n",
    "    end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=999999)\n",
    "\n",
    "    # Execute the query and stream the data\n",
    "    logs_stream = db.collection('logs').where('timestamp', '>', start_date).where('timestamp', '<', end_date).stream()\n",
    "\n",
    "    # Return the data as a list of dictionaries\n",
    "    logs_data = [log.to_dict() for log in logs_stream]\n",
    "\n",
    "    return {\n",
    "        \"logs\": logs_data\n",
    "    }\n",
    "\n",
    "def write_project(project_data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Write or update a project document to the `projects` collection in Firestore.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(project_data, dict):\n",
    "            raise ValueError(\"project_data must be a dict\")\n",
    "\n",
    "        # Required fields\n",
    "        if not project_data.get(\"project_name\") or not project_data.get(\"project_type\"):\n",
    "            raise ValueError(\"project_data must include 'project_name' and 'project_type'\")\n",
    "\n",
    "        data = project_data.copy()\n",
    "\n",
    "        # Normalize date fields if provided as strings\n",
    "        for date_field in (\"deadline\", \"created_date\", \"completed_date\"):\n",
    "            if date_field in data and isinstance(data[date_field], str):\n",
    "                try:\n",
    "                    data[date_field] = datetime.strptime(data[date_field], \"%Y-%m-%d\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Defaults\n",
    "        data.setdefault(\"status\", \"active\")\n",
    "        data.setdefault(\"actual_hours\", 0)\n",
    "        data.setdefault(\"created_date\", datetime.utcnow())\n",
    "\n",
    "        # If doc_id provided, update existing doc\n",
    "        doc_id = data.pop(\"doc_id\", None)\n",
    "        if doc_id:\n",
    "            doc_ref = db.collection(\"projects\").document(str(doc_id))\n",
    "            doc_ref.set(data, merge=True)\n",
    "            return {\"success\": True, \"doc_id\": doc_id, \"data\": data}\n",
    "        else:\n",
    "            doc_ref = db.collection(\"projects\").add(data)[1]\n",
    "            try:\n",
    "                new_doc_id = doc_ref.id\n",
    "            except Exception:\n",
    "                new_doc_id = None\n",
    "            return {\"success\": True, \"doc_id\": new_doc_id, \"data\": data}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def read_projects(filter_by: Optional[dict] = None, limit: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Read projects from the `projects` collection.\n",
    "\n",
    "    Args:\n",
    "        filter_by (dict): optional mapping of field -> value to filter with equality.\n",
    "            e.g. { 'status': 'active', 'project_type': 'tech' }\n",
    "        limit (int): maximum number of documents to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: { 'success': bool, 'projects': [dict,...], 'error': str (if any) }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coll = db.collection(\"projects\")\n",
    "        query = coll\n",
    "        if filter_by:\n",
    "            if not isinstance(filter_by, dict):\n",
    "                raise ValueError(\"filter_by must be a dict mapping field->value\")\n",
    "            for k, v in filter_by.items():\n",
    "                query = query.where(k, \"==\", v)\n",
    "\n",
    "        docs = query.limit(limit).stream()\n",
    "        projects = []\n",
    "        for d in docs:\n",
    "            doc = d.to_dict()\n",
    "            doc[\"_doc_id\"] = d.id\n",
    "            projects.append(doc)\n",
    "\n",
    "        return {\"success\": True, \"projects\": projects}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def write_category(category_data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Write or update a category document to the `categories` collection in Firestore.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(category_data, dict):\n",
    "            raise ValueError(\"category_data must be a dict\")\n",
    "\n",
    "        if not category_data.get(\"category_name\"):\n",
    "            raise ValueError(\"category_data must include 'category_name'\")\n",
    "\n",
    "        data = category_data.copy()\n",
    "\n",
    "        if \"is_active\" in data:\n",
    "            try:\n",
    "                data[\"is_active\"] = bool(int(data[\"is_active\"])) if isinstance(data[\"is_active\"], (str, int)) else bool(data[\"is_active\"])\n",
    "            except Exception:\n",
    "                data[\"is_active\"] = True\n",
    "        else:\n",
    "            data.setdefault(\"is_active\", True)\n",
    "\n",
    "        doc_id = data.pop(\"doc_id\", None)\n",
    "        if doc_id:\n",
    "            doc_ref = db.collection(\"categories\").document(str(doc_id))\n",
    "            doc_ref.set(data, merge=True)\n",
    "            return {\"success\": True, \"doc_id\": doc_id, \"data\": data}\n",
    "        else:\n",
    "            add_result = db.collection(\"categories\").add(data)\n",
    "            new_doc_id = None\n",
    "            try:\n",
    "                new_doc_ref = add_result[0]\n",
    "                new_doc_id = new_doc_ref.id\n",
    "            except Exception:\n",
    "                try:\n",
    "                    new_doc_id = add_result.id\n",
    "                except Exception:\n",
    "                    new_doc_id = None\n",
    "            return {\"success\": True, \"doc_id\": new_doc_id, \"data\": data}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def read_categories(filter_by: Optional[dict] = None, limit: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Read categories from the `categories` collection.\n",
    "\n",
    "    Args:\n",
    "        filter_by (dict): optional mapping of field -> value to filter with equality.\n",
    "            e.g. { 'is_active': True }\n",
    "        limit (int): maximum number of documents to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: { 'success': bool, 'categories': [dict,...], 'error': str (if any) }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coll = db.collection(\"categories\")\n",
    "        query = coll\n",
    "        if filter_by:\n",
    "            if not isinstance(filter_by, dict):\n",
    "                raise ValueError(\"filter_by must be a dict mapping field->value\")\n",
    "            for k, v in filter_by.items():\n",
    "                query = query.where(k, \"==\", v)\n",
    "\n",
    "        docs = query.limit(limit).stream()\n",
    "        categories = []\n",
    "        for d in docs:\n",
    "            doc = d.to_dict()\n",
    "            doc[\"_doc_id\"] = d.id\n",
    "            categories.append(doc)\n",
    "\n",
    "        return {\"success\": True, \"categories\": categories}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def write_task(task_data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Write or update a task (milestone) document to the `project_tasks` collection in Firestore.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(task_data, dict):\n",
    "            raise ValueError(\"task_data must be a dict\")\n",
    "\n",
    "        if not task_data.get(\"project_id\") or not task_data.get(\"milestone_name\"):\n",
    "            raise ValueError(\"task_data must include 'project_id' and 'milestone_name'\")\n",
    "\n",
    "        data = task_data.copy()\n",
    "\n",
    "        for date_field in (\"due_date\", \"completed_date\"):\n",
    "            if date_field in data and isinstance(data[date_field], str):\n",
    "                try:\n",
    "                    data[date_field] = datetime.strptime(data[date_field], \"%Y-%m-%d\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        data.setdefault(\"status\", \"pending\")\n",
    "\n",
    "        doc_id = data.pop(\"doc_id\", None)\n",
    "        if doc_id:\n",
    "            doc_ref = db.collection(\"project_tasks\").document(str(doc_id))\n",
    "            doc_ref.set(data, merge=True)\n",
    "            return {\"success\": True, \"doc_id\": doc_id, \"data\": data}\n",
    "        else:\n",
    "            add_result = db.collection(\"project_tasks\").add(data)\n",
    "            new_doc_id = None\n",
    "            try:\n",
    "                new_doc_ref = add_result[0]\n",
    "                new_doc_id = new_doc_ref.id\n",
    "            except Exception:\n",
    "                try:\n",
    "                    new_doc_id = add_result.id\n",
    "                except Exception:\n",
    "                    new_doc_id = None\n",
    "\n",
    "            return {\"success\": True, \"doc_id\": new_doc_id, \"data\": data}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def read_tasks(filter_by: Optional[dict] = None, limit: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Read tasks (milestones) from the `project_tasks` collection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coll = db.collection(\"project_tasks\")\n",
    "        query = coll\n",
    "        if filter_by:\n",
    "            if not isinstance(filter_by, dict):\n",
    "                raise ValueError(\"filter_by must be a dict mapping field->value\")\n",
    "            for k, v in filter_by.items():\n",
    "                query = query.where(k, \"==\", v)\n",
    "\n",
    "        docs = query.limit(limit).stream()\n",
    "        tasks = []\n",
    "        for d in docs:\n",
    "            doc = d.to_dict()\n",
    "            doc[\"_doc_id\"] = d.id\n",
    "            tasks.append(doc)\n",
    "\n",
    "        return {\"success\": True, \"tasks\": tasks}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Root Agent defined.\n"
     ]
    }
   ],
   "source": [
    "root_agent = Agent(\n",
    "    name='root_agent',\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    description='Lanas personal assistant',\n",
    "    instruction=\"\"\"You are a productivity log analyst and objective structural evaluator. \n",
    "    Your sole purpose is to analyze the Persistence Store data using your tools and provide objective assessments.\n",
    "    \n",
    "    If asked about commitment status, run the 'query_persistence' tool to check the Contradiction Flag.\n",
    "    If asked for a summary (daily, weekly, monthly), run the 'make_a_report' tool and provide structural insights based on logged avoidance patterns and corrective actions.\n",
    "    Your output must be non-judgmental and focused on systemic improvement.\n",
    "    \"\"\",\n",
    "    tools=[\n",
    "        log_persistence,\n",
    "        AgentTool(agent=log_analyst_agent),\n",
    "        query_persistence,\n",
    "        make_a_report,\n",
    "        write_project,\n",
    "        read_projects,\n",
    "        write_task,\n",
    "        read_tasks,\n",
    "        write_category,\n",
    "        read_categories,\n",
    "    ]\n",
    ")\n",
    "print(\"âœ… Root Agent defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff2fc1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Runner created.\n"
     ]
    }
   ],
   "source": [
    "runner = InMemoryRunner(agent=root_agent)\n",
    "\n",
    "print(\"âœ… Runner created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ff6832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Continue session: debug_session_id\n",
      "\n",
      "User > m tired and missed most focus tasks in afternoon feel too tired to study.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_19480\\3276699925.py:44: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  data_to_log['timestamp'] = datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Logged data to Firestore: Energy Levels\n",
      "root_agent > The log entry regarding your afternoon energy levels and missed focus tasks has been recorded.\n"
     ]
    }
   ],
   "source": [
    "response = await runner.run_debug(\"m tired and missed most focus tasks in afternoon feel too tired to study.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the log_analyst_agent\n",
    "log_analyst_runner = InMemoryRunner(agent=log_analyst_agent)\n",
    "_ = await log_analyst_runner.run_debug(\n",
    "    \"what was completed today from 2025-11-23 to 2025-11-24\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "390c4c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Continue session: debug_session_id\n",
      "\n",
      "User > can you provide a summary of my productivity logs for the past week?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_19480\\3067122689.py:47: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_19480\\3067122689.py:55: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return (datetime.utcnow() - timedelta(days=n)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_19480\\3067122689.py:89: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  logs_stream = db.collection('logs').where('timestamp', '>', start_date).where('timestamp', '<', end_date).stream()\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_19480\\3067122689.py:47: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_19480\\3067122689.py:55: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return (datetime.utcnow() - timedelta(days=n)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_19480\\3067122689.py:89: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  logs_stream = db.collection('logs').where('timestamp', '>', start_date).where('timestamp', '<', end_date).stream()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_agent > I'm sorry, but there were no productivity logs found for the past week.\n"
     ]
    }
   ],
   "source": [
    "response = await runner.run_debug(\"can you provide a summary of my productivity logs for the past week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await runner.run_debug(\"can you provide a summary of my productivity logs for the past week?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "about_Lana_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
